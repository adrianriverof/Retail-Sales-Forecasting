{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d60257",
   "metadata": {},
   "source": [
    "# Packing the process in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8413a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality(x):\n",
    "    \n",
    "    \n",
    "    #modify types\n",
    "    temp = x.astype({'month': 'O', 'wday': 'O'})\n",
    "    \n",
    "    # nulls\n",
    "    temp.event_name_1.fillna('No_event', inplace=True)\n",
    "    \n",
    "    # impute by mode\n",
    "    temp.sell_price = temp.groupby('item_id')['sell_price'].transform(lambda x: x.fillna(x.mode()[0]))\n",
    "\n",
    "    return temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f849f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variables(df):\n",
    "    \n",
    "    #### Intermitent demand\n",
    "    \n",
    "    def out_of_stock(sales, n = 5):\n",
    "        zero_sales = pd.Series(np.where(sales == 0,1,0))\n",
    "        num_zeros = zero_sales.rolling(n).sum()\n",
    "        out_of_stock = np.where(num_zeros == n,1,0)\n",
    "        return(out_of_stock)\n",
    "    \n",
    "    df = df.sort_values(by = ['store_id','item_id','date'])\n",
    "    df['out_of_stock_3'] = df.groupby(['store_id','item_id']).sales.transform(lambda x: out_of_stock(x, 3))\n",
    "    df['out_of_stock_7'] = df.groupby(['store_id','item_id']).sales.transform(lambda x: out_of_stock(x, 7))\n",
    "    df['out_of_stock_15']= df.groupby(['store_id','item_id']).sales.transform(lambda x: out_of_stock(x, 15))\n",
    "\n",
    "    #### LAGS\n",
    "    \n",
    "    def create_lags(df, variable, num_lags = 7):\n",
    "        # create object dataframe\n",
    "        lags = pd.DataFrame()\n",
    "\n",
    "        # create lags\n",
    "        for each in range(1,num_lags+1):\n",
    "            lags[variable + '_lag_'+str(each)] = df[variable].shift(each)\n",
    "\n",
    "        #return lags dataframe\n",
    "        return lags\n",
    "\n",
    "    # sell price ---> 7 days lag\n",
    "    lags_sell_price_df = (df.groupby(['store_id', 'item_id'])\n",
    "                            .apply(lambda x: create_lags(df = x, variable = 'sell_price', num_lags= 7), include_groups=False)\n",
    "                            .reset_index()\n",
    "                            .set_index('date'))\n",
    "\n",
    "    # out_of_stock ---> 1 day lag\n",
    "    lags_out_of_stock_3_df = (df.groupby(['store_id','item_id'])\n",
    "                                .apply(lambda x: create_lags(df = x, variable = 'out_of_stock_3', num_lags= 1), include_groups=False)\n",
    "                                .reset_index()\n",
    "                                .set_index('date'))\n",
    "    lags_out_of_stock_7_df = (df.groupby(['store_id','item_id'])\n",
    "                                .apply(lambda x: create_lags(df = x, variable = 'out_of_stock_7', num_lags= 1), include_groups=False)\n",
    "                                .reset_index()\n",
    "                                .set_index('date'))\n",
    "    lags_out_of_stock_15_df = (df.groupby(['store_id','item_id'])\n",
    "                                .apply(lambda x: create_lags(df = x, variable = 'out_of_stock_15', num_lags= 1), include_groups=False)\n",
    "                                .reset_index()\n",
    "                                .set_index('date'))\n",
    "\n",
    "    # sales ----> 15 days lag\n",
    "    lags_sales_df = (df.groupby(['store_id','item_id'])\n",
    "                        .apply(lambda x: create_lags(df = x, variable = 'sales', num_lags= 15))\n",
    "                        .reset_index()\n",
    "                        .set_index('date'))\n",
    "\n",
    "\n",
    "    #### ROLLING WINDOWS\n",
    "    \n",
    "    \n",
    "    def local_min(df, variable, num_periods = 7):\n",
    "        lmin = pd.DataFrame()\n",
    "\n",
    "        for each in range(2,num_periods+1):\n",
    "            lmin[variable+'_minlocal_'+str(each)] = df[variable].shift(1).rolling(each).min()\n",
    "\n",
    "        return lmin\n",
    "\n",
    "    def local_max(df, variable, num_periods = 7):\n",
    "        lmax = pd.DataFrame()\n",
    "\n",
    "        for each in range(2, num_periods+1):\n",
    "            lmax[variable+'_maxlocal_'+str(each)] = df[variable].shift(1).rolling(each).max()\n",
    "\n",
    "        return lmax\n",
    "\n",
    "    def local_mean(df, variable, num_periods = 7):\n",
    "        lmean = pd.DataFrame()\n",
    "\n",
    "        for each in range(2,num_periods+1):\n",
    "            lmean[variable+'_meanlocal_'+str(each)] = df[variable].shift(1).rolling(each).mean()\n",
    "\n",
    "        return lmean\n",
    "\n",
    "\n",
    "    min_local_df = (df.groupby(['store_id','item_id'])\n",
    "                  .apply(lambda x: local_min(df = x, variable = 'sales', num_periods= 15))\n",
    "                  .reset_index()\n",
    "                  .set_index('date'))\n",
    "    mean_local_df = (df.groupby(['store_id','item_id'])\n",
    "                        .apply(lambda x: local_mean(df = x, variable = 'sales', num_periods= 15))\n",
    "                        .reset_index()\n",
    "                        .set_index('date'))\n",
    "    max_local_df = (df.groupby(['store_id','item_id'])\n",
    "                        .apply(lambda x: local_max(df = x, variable = 'sales', num_periods= 15))\n",
    "                        .reset_index()\n",
    "                        .set_index('date'))\n",
    "\n",
    "    #### JOIN DATAFRAMES\n",
    "    \n",
    "    df_joined = pd.concat([df,\n",
    "                      lags_sell_price_df,\n",
    "                      lags_out_of_stock_3_df,\n",
    "                      lags_out_of_stock_7_df,\n",
    "                      lags_out_of_stock_15_df,\n",
    "                      lags_sales_df,\n",
    "                      min_local_df,\n",
    "                      mean_local_df,\n",
    "                      max_local_df], axis = 1)\n",
    "\n",
    "    # delete duplicated columns\n",
    "    df_joined = df_joined.loc[:,~df_joined.columns.duplicated()]\n",
    "    df_joined.dropna(inplace=True)\n",
    "    \n",
    "    # delete original variables, we already have used them to build new variables\n",
    "    \n",
    "    df_joined.drop(columns = ['sell_price','out_of_stock_3','out_of_stock_7','out_of_stock_15'],\n",
    "                  inplace=True)\n",
    "    \n",
    "    # Create a single variable for the product\n",
    "    df_joined.insert(loc=0, column = \"product\", value = df_joined.store_id + '_' + df_joined.item_id)\n",
    "    df_joined = df_joined.drop(columns = ['store_id','item_id'])\n",
    "    \n",
    "    \n",
    "    return df_joined\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60874a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_variables(x,y=None,mode = 'training'):\n",
    "    \n",
    "    '''\n",
    "    This works both for training and for execution\n",
    "    Parameter y is optional because isn't used in execution\n",
    "    \n",
    "    \n",
    "    Training: apply fit_transform and saves the objects\n",
    "    Execution: loads the objects and applies only transform\n",
    "    '''\n",
    "    \n",
    "    x.reset_index(inplace = True)\n",
    "\n",
    "    # ENCODERS\n",
    "    path_ohe = '../../04_Models/ohe_retail.pickle' \n",
    "    path_te  = '../../04_Models/te_retail.pickle' \n",
    "    \n",
    "    \n",
    "    #ONE HOT ENCODING\n",
    "    var_ohe = ['event_name_1']\n",
    "    if mode == 'training':\n",
    "        \n",
    "        ohe = OneHotEncoder(sparse = False, handle_unknown='ignore')\n",
    "        \n",
    "        \n",
    "        ohe_x = ohe.fit_transform(x[var_ohe])\n",
    "        ohe_x = pd.DataFrame(ohe_x, columns = ohe.get_feature_names_out())\n",
    "        with open(path_ohe, mode='wb') as file:\n",
    "            pickle.dump(ohe, file)\n",
    "    else:\n",
    "        # execution mode\n",
    "        with open(path_ohe, mode='rb') as file:\n",
    "            ohe = pickle.load(file)\n",
    "        ohe_x = ohe.transform(x[var_ohe])\n",
    "        ohe_x = pd.DataFrame(ohe_x, columns = ohe.get_feature_names_out())\n",
    "\n",
    "    #TARGET ENCODING    \n",
    "    var_te = ['month','wday','weekday']\n",
    "    if mode == 'training':\n",
    "        \n",
    "        #Make sure Y is as long as X\n",
    "        y.reset_index(inplace = True, drop = True)\n",
    "        y = y.loc[y.index.isin(x.index)]\n",
    "        \n",
    "        # Training mode\n",
    "        te = TargetEncoder(min_samples_leaf=100, return_df = False)\n",
    "        te_x = te.fit_transform(x[var_te], y = y)\n",
    "        names_te = [variable + '_te' for variable in var_te]\n",
    "        te_x = pd.DataFrame(te_x, columns = names_te)\n",
    "        with open(path_te, mode='wb') as file:\n",
    "            pickle.dump(te, file)\n",
    "    else:\n",
    "        # training mode\n",
    "        with open(path_te, mode='rb') as file:\n",
    "            te = pickle.load(file)\n",
    "        te_x = te.transform(x[var_te])\n",
    "        names_te = [variable + '_te' for variable in var_te]\n",
    "        te_x = pd.DataFrame(te_x, columns = names_te)\n",
    "    \n",
    "      \n",
    "    # CLEANUP\n",
    "    #eliminate originals\n",
    "    x = x.drop(columns=['event_name_1','month','wday','weekday'])\n",
    "    # include the other dataframes\n",
    "    x = pd.concat([x,ohe_x,te_x], axis=1).set_index('date')\n",
    "\n",
    "    # output\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d613112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_preselection(x,y):\n",
    "    \n",
    "        # only during training\n",
    "    \n",
    "    #Delete product and index\n",
    "    x.reset_index(drop = True,inplace = True)\n",
    "    x.drop(columns='product',inplace = True)\n",
    "    \n",
    "    # make sure equal lenght\n",
    "    y = y.loc[y.index.isin(x.index)]\n",
    "    \n",
    "\n",
    "    mutual_selector = mutual_info_regression(x,y)\n",
    "    pos_var_limit = 70\n",
    "    ranking_mi = pd.DataFrame(mutual_selector, index = x.columns).reset_index()\n",
    "    ranking_mi.columns = ['variable','importance_mi']\n",
    "    ranking_mi = ranking_mi.sort_values(by = 'importance_mi', ascending = False)\n",
    "    ranking_mi['ranking_mi'] = np.arange(0,ranking_mi.shape[0])\n",
    "    enter_mi = ranking_mi.iloc[0:pos_var_limit].variable\n",
    "    x_mi = x[enter_mi].copy()\n",
    "\n",
    "    return(x_mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5725ae",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80158c65",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ddc354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_each(x_product, y):\n",
    "          \n",
    "        # individual modeling\n",
    "        # gets x and y of a product\n",
    "        # finds optimal parameters \n",
    "        # returns best model\n",
    "        \n",
    "        \n",
    "    # exclude product as model variable\n",
    "    var_to_model = x_product.columns.to_list()[2:]\n",
    "    \n",
    "    #Define cross validation\n",
    "    time_cv = TimeSeriesSplit(3, test_size = 8)\n",
    "    \n",
    "    # define algorythm pipe\n",
    "    pipe = Pipeline([('algorythm', HistGradientBoostingRegressor())])\n",
    "    \n",
    "    grid = [ \n",
    "         {'algorythm': [HistGradientBoostingRegressor()]#, # after some tries, default works the best\n",
    "#         'algorythm__learning_rate': [0.01,0.025,0.05,0.1],\n",
    "#         'algorythm__max_iter': [50,100,200],\n",
    "#         'algorythm__max_depth': [None, 5,10,20],\n",
    "#         'algorythm__min_samples_leaf': [20, 500],\n",
    "#         'algorythm__l2_regularization': [0,0.25,0.5,0.75,1]\n",
    "        }\n",
    "                       \n",
    "    ]\n",
    "           \n",
    "    #create the models\n",
    "    random_search = RandomizedSearchCV(estimator = pipe,\n",
    "                                       param_distributions = grid, \n",
    "                                       n_iter = 1, #30, \n",
    "                                       cv = time_cv, \n",
    "                                       scoring = 'neg_mean_absolute_error', \n",
    "                                       verbose = 0,\n",
    "                                       n_jobs = -1)\n",
    "\n",
    "    \n",
    "    model = random_search.fit(x_product[var_to_model],y)\n",
    "    \n",
    "    final_model = model.best_estimator_.fit(x_product[var_to_model],y)\n",
    "    \n",
    "    return(final_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3598561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_training(df):\n",
    "    \n",
    "        # goes through all products\n",
    "        # models them\n",
    "        # creates a list with all the models for all the producs\n",
    "\n",
    "        # gets the dataframe clean and segmented by product\n",
    "        \n",
    "        # doesn't return anything, \n",
    "        # just saves in disk the trained object with all the models\n",
    "        \n",
    "        \n",
    "    products_list = list(df['product'].unique())\n",
    "    models_list = []\n",
    "    \n",
    "    for each in products_list:\n",
    "\n",
    "        product = each #[0]\n",
    "        target = 'sales'\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = df.loc[df['product'] == product].drop(columns=target).copy()\n",
    "        y = df.loc[df['product'] == product,'sales'].copy()\n",
    "\n",
    "        x = transform_variables(x,y)\n",
    "        x = variable_preselection(x,y)\n",
    "\n",
    "        # Call to model\n",
    "        model = model_each(x,y)\n",
    "        \n",
    "        # add final model to the list\n",
    "        models_list.append((product,model))\n",
    "    \n",
    "    #Save trained models list\n",
    "    \n",
    "    models_path = '../../04_Models/models_list_retail.pickle'\n",
    "    with open(models_path, mode='wb') as file:\n",
    "        pickle.dump(models_list, file)\n",
    "    \n",
    "    print('Successfully trained')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0000a",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3bf35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def launch_execution(df):\n",
    "    \n",
    "    # makes the forecast for each product\n",
    "    # but only one day\n",
    "    \n",
    "    # gets the new dataset to predict\n",
    "    # needs to have the structure of DataForProduction.csv, \n",
    "    # from the Validation folder\n",
    "   \n",
    "\n",
    "    \n",
    "    #LOAD THE MODELS\n",
    "    models_path = '../../04_Models/models_list_retail.pickle'\n",
    "    with open(models_path, mode='rb') as file:\n",
    "        models_list = pickle.load(file)\n",
    "    \n",
    "    predictions_df = pd.DataFrame(columns=['date','product','sales','prediction'])\n",
    "    \n",
    "    for each in range(0,len(models_list)):\n",
    "\n",
    "        product = models_list[each][0]\n",
    "        model = models_list[each][1]\n",
    "        variables = model[0].feature_names_in_\n",
    "        target = 'sales'\n",
    "        \n",
    "        \n",
    "        x = df.loc[df['product'] == product].drop(columns=target).copy()\n",
    "        y = df.loc[df['product'] == product,'sales'].copy()\n",
    "\n",
    "        date = df.reset_index().copy()\n",
    "        date = date.loc[date['product'] == product,'date'].values\n",
    "\n",
    "        #var transformation\n",
    "        x = transform_variables(x, mode = 'execution')\n",
    "        \n",
    "        #var selection\n",
    "        x = x[variables]\n",
    "        \n",
    "        #Calculate predictions\n",
    "        predictions = pd.DataFrame(data={'date': date,\n",
    "                                          'product': product,\n",
    "                                          'sales': y,\n",
    "                                          'prediction': model.predict(x)})\n",
    "\n",
    "        predictions['prediction'] = predictions.prediction.astype('int')\n",
    "\n",
    "        predictions_df = pd.concat([predictions_df,predictions])\n",
    "    \n",
    "    predictions_df = predictions_df.loc[predictions_df.index == predictions_df.index.min()]  \n",
    "    return(predictions_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248737ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_forecast(x):\n",
    "    \n",
    "    \n",
    "    # x is the dataframe with the DataForProduction.csv structure\n",
    "\n",
    "    # to apply the recursivity:\n",
    "        # - predict the first day (15 days from the oldest)\n",
    "        # - saves the prediction in the day and deletes the oldest day\n",
    "        # - the next iteration is going to predict the next day\n",
    "\n",
    "    \n",
    "    for each in range(0,8):\n",
    "        step1_df = data_quality(x)\n",
    "        step2_df = create_variables(step1_df)\n",
    "        \n",
    "        #predict\n",
    "        f = launch_execution(step2_df)\n",
    "        f['store_id'] = f.product.str[:4]\n",
    "        f['item_id'] = f.product.str[5:]\n",
    "\n",
    "        # Update the sales data with the prediction\n",
    "        x.loc[(x.index.isin(f.date)) & (x.store_id.isin(f.store_id)) & (x.item_id.isin(f.item_id)),'sales'] = f.prediction\n",
    "                                                              \n",
    "        # Delete the oldest day in x\n",
    "        x = x.loc[x.index != x.index.min()]\n",
    "        \n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2fa53",
   "metadata": {},
   "source": [
    "# PROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89cf553",
   "metadata": {},
   "source": [
    "### Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f975786",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully trained\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#Load data\n",
    "data_path = '../../02_Data/03_Work/work.csv'\n",
    "df = pd.read_csv(data_path,sep=',',parse_dates=['date'],index_col='date')\n",
    "\n",
    "\n",
    "final_variables = ['store_id',\n",
    "                     'item_id',\n",
    "                     'event_name_1',                     \n",
    "                     'month',\n",
    "                     'sell_price',                      \n",
    "                     'wday',\n",
    "                     'weekday',\n",
    "                     'sales']\n",
    "\n",
    "df = df[final_variables]\n",
    "\n",
    "\n",
    "step1_df = data_quality(df)\n",
    "step2_df = create_variables(step1_df)\n",
    "\n",
    "\n",
    "launch_training(step2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11589cc",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9753fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE =  5.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>product</th>\n",
       "      <th>sales</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_3_FOODS_3_090</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_3_FOODS_3_120</td>\n",
       "      <td>52</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_3_FOODS_3_202</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_3_FOODS_3_252</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_3_FOODS_3_288</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_3_FOODS_3_329</td>\n",
       "      <td>64</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_3_FOODS_3_555</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_3_FOODS_3_586</td>\n",
       "      <td>76</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_3_FOODS_3_587</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_3_FOODS_3_714</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_4_FOODS_3_090</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_4_FOODS_3_120</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_4_FOODS_3_202</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_4_FOODS_3_252</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_4_FOODS_3_288</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_4_FOODS_3_329</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_4_FOODS_3_555</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_4_FOODS_3_586</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_4_FOODS_3_587</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-16</th>\n",
       "      <td>2015-12-16</td>\n",
       "      <td>CA_4_FOODS_3_714</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date           product sales prediction\n",
       "2015-12-16 2015-12-16  CA_3_FOODS_3_090     0         -9\n",
       "2015-12-16 2015-12-16  CA_3_FOODS_3_120    52         53\n",
       "2015-12-16 2015-12-16  CA_3_FOODS_3_202    20         13\n",
       "2015-12-16 2015-12-16  CA_3_FOODS_3_252    36         33\n",
       "2015-12-16 2015-12-16  CA_3_FOODS_3_288    35         24\n",
       "2015-12-16 2015-12-16  CA_3_FOODS_3_329    64         44\n",
       "2015-12-16 2015-12-16  CA_3_FOODS_3_555    30         30\n",
       "2015-12-16 2015-12-16  CA_3_FOODS_3_586    76         62\n",
       "2015-12-16 2015-12-16  CA_3_FOODS_3_587    29         30\n",
       "2015-12-16 2015-12-16  CA_3_FOODS_3_714    19         17\n",
       "2015-12-16 2015-12-16  CA_4_FOODS_3_090     0          0\n",
       "2015-12-16 2015-12-16  CA_4_FOODS_3_120    16          3\n",
       "2015-12-16 2015-12-16  CA_4_FOODS_3_202    11          9\n",
       "2015-12-16 2015-12-16  CA_4_FOODS_3_252     5          7\n",
       "2015-12-16 2015-12-16  CA_4_FOODS_3_288     3          6\n",
       "2015-12-16 2015-12-16  CA_4_FOODS_3_329    10          5\n",
       "2015-12-16 2015-12-16  CA_4_FOODS_3_555     4          1\n",
       "2015-12-16 2015-12-16  CA_4_FOODS_3_586    10          9\n",
       "2015-12-16 2015-12-16  CA_4_FOODS_3_587     5         11\n",
       "2015-12-16 2015-12-16  CA_4_FOODS_3_714    11         10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "#Load data\n",
    "data_path = '../../02_Data/02_Validation/validation.csv'\n",
    "df = pd.read_csv(data_path,sep=',',parse_dates=['date'],index_col='date')\n",
    "\n",
    "\n",
    "#Select used variables\n",
    "final_variables = ['store_id',\n",
    "                     'item_id',\n",
    "                     'event_name_1',                     \n",
    "                     'month',\n",
    "                     'sell_price',                      \n",
    "                     'wday',\n",
    "                     'weekday',\n",
    "                     'sales']\n",
    "\n",
    "\n",
    "df = df[final_variables]\n",
    "\n",
    "step1_df = data_quality(df)\n",
    "step2_df = create_variables(step1_df)\n",
    "\n",
    "forecast_1day = launch_execution(step2_df)\n",
    "\n",
    "print('MAE = ', mean_absolute_error(forecast_1day['sales'],forecast_1day['prediction']))\n",
    "\n",
    "forecast_1day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a9fc49",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
